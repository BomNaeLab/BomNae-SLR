{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd290bccd2f228",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "data_dir = 'signData'\n",
    "\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "val_dir = os.path.join(data_dir,'valid')\n",
    "\n",
    "output_dir = os.path.join(data_dir,'nptxt_lite')\n",
    "weight_dir = os.path.join(data_dir,'weights')\n",
    "val_output_dir = os.path.join(data_dir,\"nptxt_lite_val\")\n",
    "\n",
    "train_landmark_dir = os.path.join(train_dir,'label','landmark')\n",
    "train_morpheme_dir = os.path.join(train_dir,'label','morpheme')\n",
    "\n",
    "val_landmark_dir = os.path.join(val_dir,'landmark')\n",
    "val_morpheme_dir = os.path.join(val_dir,'morpheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463029f1b6f6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_calc(F, a, b, x):\n",
    "    fps=30\n",
    "    a_frame = a * fps  # a를 프레임 단위로 변환\n",
    "    b_frame = b * fps  # b를 프레임 단위로 변환\n",
    "    x_frame = x * fps  # x(영상의 길이)를 프레임 단위로 변환\n",
    "    if F < a_frame:\n",
    "        return 0\n",
    "    elif a_frame <= F < a_frame + 2 * fps:\n",
    "        return (F - a_frame) / (2 * fps)\n",
    "    elif a_frame + 2 * fps <= F < b_frame:\n",
    "        return 1\n",
    "    elif b_frame <= F <= x_frame:\n",
    "        return 1 - ((F - b_frame) / (x_frame - b_frame)) * 0.5\n",
    "    else:\n",
    "        return 0.5\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f255a616e3863c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train weight data\n",
    "# start_person=\"01\"\n",
    "# start_word=\"0001\"\n",
    "# #사람별 데이터(01~16)\n",
    "# for person in os.listdir(train_landmark_dir):\n",
    "#     #중간부터 다시 데이터 변환시 체크포인트\n",
    "#     if int(person) < int(start_person):\n",
    "#         continue\n",
    "        \n",
    "#     #변환 데이터 저장시 사람 폴더 생성(01~16)\n",
    "#     person_output_path = os.path.join(weight_dir, str(int(person)))\n",
    "#     os.makedirs(person_output_path, exist_ok=True)\n",
    "    \n",
    "#     #각 단어별 좌표값 및 단어 뜻 순회. \n",
    "#     for word_coords, word_morpheme in zip(os.listdir(os.path.join(train_landmark_dir, person)), os.listdir(os.path.join(train_morpheme_dir, person))):\n",
    "#         #중간부터 다시 데이터 변환시 체크 포인트\n",
    "#         if int(word_coords[11:15]) < int(start_word):\n",
    "#             continue\n",
    "#         #정면카메라 외 다른 방향 카메라도 3d좌표값은 동일하여 한개의 방향에서만 데이터 추출\n",
    "#         #단어 별 뜻, 단어의 할당 넘버, 영상 길이와 영상 내 수어 구간 추출\n",
    "#         if \"F\" in word_morpheme:\n",
    "#             file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "#             morpheme_file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "#             with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "#                 data = json.load(morpheme_file)\n",
    "#                 try:\n",
    "#                     num = data['metaData']['name'][11:15]\n",
    "#                     start_sign = data[\"data\"][0][\"start\"]\n",
    "#                     end_sign = data[\"data\"][0][\"end\"]\n",
    "#                     duration = data[\"metaData\"][\"duration\"]\n",
    "#                 #결측치 오류 검출   \n",
    "#                 except IndexError as e:\n",
    "#                     name = False\n",
    "#                     print(f\"Error reading {morpheme_file_path}: {e}\")\n",
    "#                     continue\n",
    "        \n",
    "#         #각 단어 별 좌표 순회시, 단어 뜻이 결측치인 데이터는 제외\n",
    "#         if \"F\" in word_coords and name is not False:\n",
    "#             wordWeight = []\n",
    "#             #단어 영상의 각 프레임별 좌표값 추출\n",
    "#             for frame in os.listdir(os.path.join(train_landmark_dir, person, word_coords)):\n",
    "#                 frame_num = int(frame[25:37])\n",
    "#                 # 가중치 계산만 수행\n",
    "#                 wordWeight.append(weight_calc(frame_num, start_sign, end_sign, duration))\n",
    "            \n",
    "#             # 결과를 저장\n",
    "#             time = [start_sign, end_sign, duration]\n",
    "#             word_output_path = os.path.join(person_output_path, f'{num}.npz')\n",
    "#             np.savez(word_output_path, weight=wordWeight, time=time)\n",
    "#             print(f\"Saved weights for {word_output_path}\")\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e83bda6105c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "def pre_data(landmark_dir,morpheme_dir,output_dir):\n",
    "        import numpy as np\n",
    "        start_person=\"01\"\n",
    "        start_word=0\n",
    "        #사람별 데이터(01~16)\n",
    "        for person in sorted(os.listdir(landmark_dir)):\n",
    "            #중간부터 다시 데이터 변환시 체크포인트\n",
    "            if int(person) < int(start_person):\n",
    "                continue\n",
    "                \n",
    "            #변환 데이터 저장시 사람 폴더 생성(01~16)\n",
    "            person_output_path = os.path.join(output_dir, str(int(person)))\n",
    "            os.makedirs(person_output_path, exist_ok=True)\n",
    "            \n",
    "            #각 단어별 좌표값 및 단어 뜻 순회. \n",
    "            for word_coords,word_morpheme in zip(sorted(os.listdir(os.path.join(landmark_dir, person))),sorted(os.listdir(os.path.join(morpheme_dir, person)))):\n",
    "                #중간부터 다시 데이터 변환시 체크 포인트\n",
    "                if int(word_coords[11:15]) <start_word:\n",
    "                    continue\n",
    "                elif int(word_coords[11:15]) == start_word:\n",
    "                    start_word=0\n",
    "                #정면카메라 외 다른 방향 카메라도 3d좌표값은 동일하여 한개의 방향에서만 데이터 추출\n",
    "                #단어 별 뜻, 단어의 할당 넘버,영상 길이와 영상 내 수어 구간 추출\n",
    "                file_path = os.path.join(morpheme_dir, person, word_morpheme)\n",
    "                morpheme_file_path = os.path.join(morpheme_dir, person, word_morpheme)\n",
    "                with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "                    data = json.load(morpheme_file)\n",
    "                    try:\n",
    "                        name = data['data'][0]['attributes'][0]['name']\n",
    "                        num = data['metaData']['name'][11:15]\n",
    "                        start_sign = data[\"data\"][0][\"start\"]\n",
    "                        end_sign = data[\"data\"][0][\"end\"]\n",
    "                        duration = data[\"metaData\"][\"duration\"]\n",
    "                    #결측치 오류 검출   \n",
    "                    except IndexError as e:\n",
    "                        name=False\n",
    "                        print(f\"Error reading {morpheme_file_path}: {e}\")\n",
    "                        continue\n",
    "                #각 단어 별 좌표 순회시, 단어 뜻이 결측치인 데이터는 제외\n",
    "                if name is not False:\n",
    "                    wordCoordL = np.empty((0, 4, 5, 3))  \n",
    "                    wordCoordR = np.empty((0, 4, 5, 3))\n",
    "                    wordCoordP = np.empty((0, 3, 10))\n",
    "                    wordWeight = []\n",
    "                    #단어 영상의 각 프레임별 좌표값 추출            \n",
    "                    for frame in sorted(os.listdir(os.path.join(landmark_dir, person, word_coords))):\n",
    "                        \n",
    "                        file_path = os.path.join(landmark_dir, person, word_coords, frame)\n",
    "                        frame_num=int(frame[25:37])\n",
    "                        with open(file_path, 'r') as json_file:\n",
    "                            data = json.load(json_file)\n",
    "                            lh_points = data['people']['hand_left_keypoints_3d']\n",
    "                            rh_points = data['people']['hand_right_keypoints_3d']\n",
    "                            p_points = data['people']['pose_keypoints_3d']\n",
    "                            #포즈 좌표값 추출\n",
    "                            #1920x1080영상을 메디어파이프의 특성상 영상의 중심을 기준으로 1080x1080으로 잘라서 기존 죄표를 픽셀 값으로 변환 후 \n",
    "                            # 다시 미디어 파이프와 같은 스케일인 0~1사이 값으로 정규화\n",
    "                            #z좌표는 미디어파이프의 기준이 되는 랜드마크의 좌표 활용하여 계산\n",
    "                            \n",
    "                            preFrameCoordP = np.array([[(960 * p_points[i] + 960), #중앙 랜드마크 x 0.5되도록  \n",
    "                                                        (1080 * p_points[i + 1] + 540),\n",
    "                                                        (p_points[32 + 2] - p_points[i + 2]) / 10]\n",
    "                                                    for i in range(0, len(p_points), 4)], dtype=np.float32)\n",
    "\n",
    "                            preFrameCoordL = np.array([[(960 * lh_points[i] + 960),\n",
    "                                                        (1080 * lh_points[i + 1] + 540),\n",
    "                                                        (lh_points[2] - lh_points[i + 2]) / 10]\n",
    "                                                    for i in range(4, len(lh_points), 4)], dtype=np.float32)\n",
    "\n",
    "                            preFrameCoordR = np.array([[(960 * rh_points[i] + 960),\n",
    "                                                        (1080 * rh_points[i + 1] + 540),\n",
    "                                                        (rh_points[2] - rh_points[i + 2]) / 10]\n",
    "                                                    for i in range(4, len(rh_points), 4)], dtype=np.float32)\n",
    "                            \n",
    "                            \n",
    "                            #포즈 좌표에서 하반신 랜드마크 제외\n",
    "                            preFrameCoordP = preFrameCoordP[[i for i in range(19) if (0 <= i <= 7) or (17 <= i <= 18)]]\n",
    "                            \n",
    "                            #랜드마크 값들을 모델 트레이닝에 적합한 모양으로 재배열.\n",
    "                            frameCoordL=preFrameCoordL.reshape(5,4,3).transpose(1,0,2)[::-1]\n",
    "                            frameCoordR=preFrameCoordR.reshape(5,4,3).transpose(1,0,2)[::-1]\n",
    "                            frameCoordP=preFrameCoordP.T\n",
    "\n",
    "                            import numpy as np\n",
    "\n",
    "\n",
    "                            # 1번 인덱스의 x, y 좌표\n",
    "                            x_center = frameCoordP[0][1]\n",
    "                            y_center = frameCoordP[1][1]\n",
    "\n",
    "                            # 1080x1080 크롭 영역을 정하기 위해 크롭 영역의 좌상단 좌표 계산\n",
    "                            x_min = max(0, x_center - 540)\n",
    "                            y_min = max(0, y_center - 540)\n",
    "\n",
    "                            # 크롭 영역이 1920x1080을 초과하지 않도록 조정\n",
    "                            x_min = min(x_min, 1920 - 1080)\n",
    "                            y_min = min(y_min, 1080 - 1080)\n",
    "\n",
    "                            # 크롭 영역의 우상단 좌표 계산\n",
    "                            x_max = x_min + 1080\n",
    "                            y_max = y_min + 1080\n",
    "\n",
    "                            # 좌표들을 크롭 영역에 맞게 이동\n",
    "                            frameCoordP[0] -= x_min\n",
    "                            frameCoordP[1] -= y_min\n",
    "                            frameCoordP[0] /= 1080\n",
    "                            frameCoordP[1] /= 1080\n",
    "\n",
    "                            frameCoordP[0] = np.clip(frameCoordP[0], 0, 1)\n",
    "                            frameCoordP[1] = np.clip(frameCoordP[1], 0, 1)\n",
    "\n",
    "                            \n",
    "                            def min_cal(coords):\n",
    "                                coord_x_min = np.min(coords[:, :, 0]if coords.ndim == 3 else coords[:, 0])\n",
    "                                coord_x_cal = np.max(coords[:, :, 0]if coords.ndim == 3 else coords[:, 0]) - coord_x_min \n",
    "\n",
    "                                coord_y_min = np.min(coords[:, :, 1]if coords.ndim == 3 else coords[:, 1])\n",
    "                                coord_y_cal = np.max(coords[:, :, 1]if coords.ndim == 3 else coords[:, 1]) - coord_y_min \n",
    "                                \n",
    "                                return [coord_x_min,coord_y_min,np.min(frameCoordL[:, :, 2]if coords.ndim == 3 else coords[:, 2])],[coord_x_cal,coord_y_cal]\n",
    "\n",
    "                            coordL_min,coordL_cal=min_cal(frameCoordL)\n",
    "                            coordR_min,coordR_cal=min_cal(frameCoordR)\n",
    "\n",
    "                            frameCoordL = (frameCoordL - coordL_min) / (coordL_cal[0] if coordL_cal[0] > coordL_cal[1] else coordL_cal[1])\n",
    "                            frameCoordR = (frameCoordR - coordR_min) / (coordR_cal[0] if coordR_cal[0] > coordR_cal[1] else coordR_cal[1])\n",
    "                            \n",
    "                            #한 단어의 프레임 별 좌표를 하나의 넘파이 배열로 저장\n",
    "                            wordCoordL = np.append(wordCoordL, [frameCoordL], axis=0)\n",
    "                            wordCoordR = np.append(wordCoordR, [frameCoordR], axis=0)\n",
    "                            wordCoordP = np.append(wordCoordP, [frameCoordP], axis=0)\n",
    "                            wordWeight.append(weight_calc(frame_num,start_sign,end_sign,duration))\n",
    "                            \n",
    "                    label=[name,num,start_sign,end_sign,duration]\n",
    "                    word_output_path = os.path.join(person_output_path, f'{num}.npz')\n",
    "                    np.savez(word_output_path, wordCoordL=wordCoordL, wordCoordR=wordCoordR, wordCoordP=wordCoordP, label=label, weight=wordWeight)\n",
    "                    print(f\"Saved {word_output_path}\")\n",
    "\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b458f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data(val_landmark_dir,val_morpheme_dir,val_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(frameCoordP[0], frameCoordP[1], color='blue', label=\"Transformed Points\")\n",
    "plt.scatter([frameCoordP[0][1]], [frameCoordP[1][1]], color='red', label=\"Center Point (Index 1)\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(0.5, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0.5, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.title(\"Normalized Coordinates after Cropping and Centering\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c517327bb530d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #val data\n",
    "# start_person=\"01\"\n",
    "# start_word=\"0001\"\n",
    "# #사람별 데이터(01~16)\n",
    "# for person in os.listdir(val_landmark_dir):  # train_landmark_dir -> val_landmark_dir\n",
    "#     #중간부터 다시 데이터 변환시 체크포인트\n",
    "#     if int(person) < int(start_person):\n",
    "#         continue\n",
    "        \n",
    "#     #변환 데이터 저장시 사람 폴더 생성(01~16)\n",
    "#     person_output_path = os.path.join(val_output_dir, str(int(person)))\n",
    "#     os.makedirs(person_output_path, exist_ok=True)\n",
    "    \n",
    "#     #각 단어별 좌표값 및 단어 뜻 순회. \n",
    "#     for word_coords,word_morpheme in zip(os.listdir(os.path.join(val_landmark_dir, person)),os.listdir(os.path.join(val_morpheme_dir, person))):  # train_landmark_dir -> val_landmark_dir, train_morpheme_dir -> val_morpheme_dir\n",
    "#         #중간부터 다시 데이터 변환시 체크 포인트\n",
    "#         if int(word_coords[11:15]) < int(start_word):\n",
    "#             continue\n",
    "#         #정면카메라 외 다른 방향 카메라도 3d좌표값은 동일하여 한개의 방향에서만 데이터 추출\n",
    "#         #단어 별 뜻, 단어의 할당 넘버,영상 길이와 영상 내 수어 구간 추출\n",
    "#         if \"F\" in word_morpheme:\n",
    "#             file_path = os.path.join(val_morpheme_dir, person, word_morpheme)  # train_morpheme_dir -> val_morpheme_dir\n",
    "#             morpheme_file_path = os.path.join(val_morpheme_dir, person, word_morpheme)  # train_morpheme_dir -> val_morpheme_dir\n",
    "#             with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "#                 data = json.load(morpheme_file)\n",
    "#                 try:\n",
    "#                     name = data['data'][0]['attributes'][0]['name'].replace('\\n', '')\n",
    "#                     num = data['metaData']['name'][11:15]\n",
    "#                 #결측치 오류 검출   \n",
    "#                 except IndexError as e:\n",
    "#                     name = False\n",
    "#                     print(f\"Error reading {morpheme_file_path}: {e}\")\n",
    "#                     continue\n",
    "#         #각 단어 별 좌표 순회시, 단어 뜻이 결측치인 데이터는 제외\n",
    "#         if \"F\" in word_coords and name is not False:\n",
    "#             wordCoordL = np.empty((0, 4, 5, 3))  \n",
    "#             wordCoordR = np.empty((0, 4, 5, 3))\n",
    "#             wordCoordP = np.empty((0, 3, 10))\n",
    "#             wordWeight = []\n",
    "#             #단어 영상의 각 프레임별 좌표값 추출            \n",
    "#             for frame in os.listdir(os.path.join(val_landmark_dir, person, word_coords)):  # train_landmark_dir -> val_landmark_dir\n",
    "#                 file_path = os.path.join(val_landmark_dir, person, word_coords, frame)  # train_landmark_dir -> val_landmark_dir\n",
    "#                 frame_num = int(frame[25:37])\n",
    "#                 with open(file_path, 'r') as json_file:\n",
    "#                     data = json.load(json_file)\n",
    "#                     lh_points = data['people']['hand_left_keypoints_3d']\n",
    "#                     rh_points = data['people']['hand_right_keypoints_3d']\n",
    "#                     p_points = data['people']['pose_keypoints_3d']\n",
    "#                     #포즈 좌표값 추출 및 정규화 코드 동일\n",
    "#                     preFrameCoordP = np.array([[(960 * p_points[i] + 960 - 420) / (1500 - 420), \n",
    "#                                                 (1080 * p_points[i + 1] + 540) / 1080,\n",
    "#                                                 (p_points[32 + 2] - p_points[i + 2]) / 10]\n",
    "#                                                for i in range(0, len(p_points), 4)], dtype=np.float32)\n",
    "\n",
    "#                     preFrameCoordL = np.array([[(960 * lh_points[i] + 960 - 420) / (1500 - 420),\n",
    "#                                                 (1080 * lh_points[i + 1] + 540) / 1080,\n",
    "#                                                 (lh_points[2] - lh_points[i + 2]) / 10]\n",
    "#                                                for i in range(4, len(lh_points), 4)], dtype=np.float32)\n",
    "\n",
    "#                     preFrameCoordR = np.array([[(960 * rh_points[i] + 960 - 420) / (1500 - 420),\n",
    "#                                                 (1080 * rh_points[i + 1] + 540) / 1080,\n",
    "#                                                 (rh_points[2] - rh_points[i + 2]) / 10]\n",
    "#                                                for i in range(4, len(rh_points), 4)], dtype=np.float32)\n",
    "                    \n",
    "#                     #포즈 좌표에서 하반신 랜드마크 제외\n",
    "#                     preFrameCoordP = preFrameCoordP[[i for i in range(19) if (0 <= i <= 7) or (17 <= i <= 18)]]\n",
    "                    \n",
    "#                     #랜드마크 값들을 모델 트레이닝에 적합한 모양으로 재배열.\n",
    "#                     frameCoordL = preFrameCoordL.reshape(5, 4, 3).transpose(1, 0, 2)[::-1]\n",
    "#                     frameCoordR = preFrameCoordR.reshape(5, 4, 3).transpose(1, 0, 2)[::-1]\n",
    "#                     frameCoordP = preFrameCoordP.T\n",
    "#                     #한 단어의 프레임 별 좌표를 하나의 넘파이 배열로 저장\n",
    "#                     wordCoordL = np.append(wordCoordL, [frameCoordL], axis=0)\n",
    "#                     wordCoordR = np.append(wordCoordR, [frameCoordR], axis=0)\n",
    "#                     wordCoordP = np.append(wordCoordP, [frameCoordP], axis=0)\n",
    "\n",
    "                    \n",
    "#             label = [int(person),num,name]\n",
    "#             word_output_path = os.path.join(person_output_path, f'{num}.npz')\n",
    "#             np.savez(word_output_path, wordCoordL=wordCoordL, wordCoordR=wordCoordR, wordCoordP=wordCoordP, label=label)\n",
    "#             print(f\"Saved {word_output_path}\")\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d38785c53bd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'signData'\n",
    "\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "val_dir = os.path.join(data_dir,'valid')\n",
    "\n",
    "output_dir = os.path.join(data_dir,'nptxt_CL')\n",
    "val_output_dir = os.path.join(data_dir,\"nptxt_CL_val\")\n",
    "# weight_dir = os.path.join(data_dir,'weights')\n",
    "\n",
    "train_landmark_dir = os.path.join(train_dir,'label','landmark')\n",
    "train_morpheme_dir = os.path.join(train_dir,'label','morpheme')\n",
    "\n",
    "val_landmark_dir = os.path.join(val_dir,'landmark')\n",
    "val_morpheme_dir = os.path.join(val_dir,'morpheme')\n",
    "\n",
    "\n",
    "def getoutputdir(type=\"train\"):\n",
    "    return val_output_dir if type=='val' else output_dir\n",
    "\n",
    "with open('wordtonum.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    words_dicts = json.load(json_file)\n",
    "\n",
    "def load_data(file_name,type=\"train\"):\n",
    "    path=f\"{getoutputdir(type)}/{file_name}\"\n",
    "\n",
    "    data = np.load(path)\n",
    "\n",
    "    #좌표값 로드\n",
    "    wordCoordL = data['wordCoordL']\n",
    "    wordCoordR = data['wordCoordR']\n",
    "    wordCoordP = data['wordCoordP']\n",
    "    wordWeight = data['weight']\n",
    "    #단어 뜻 호출\n",
    "    ans = data['label'][0]\n",
    "    ans = ans.replace('\\n', '')\n",
    "    #해당 단어의 value 호출\n",
    "    if ans[-1].isdigit():\n",
    "        ans = ans[:-1]\n",
    "    # label = words_dicts[ans]\n",
    "    label=ans\n",
    "    if type=='train':\n",
    "        return wordCoordL, wordCoordR, wordCoordP, label ,wordWeight\n",
    "    else:\n",
    "        return wordCoordL, wordCoordR, wordCoordP, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_word(person, start, num):\n",
    "    coordLs = []\n",
    "    coordRs = []\n",
    "    coordPs = []\n",
    "    labels = []\n",
    "    checks = []\n",
    "    for WNum in range(start, start + num):\n",
    "        wordCoordL, wordCoordR, wordCoordP, label = load_data(person, WNum)\n",
    "        coordLs.append(wordCoordL)\n",
    "        coordRs.append(wordCoordR)\n",
    "        coordPs.append(wordCoordP)\n",
    "        labels.append(int(label[0]))\n",
    "        checks.append((label[1:3]))\n",
    "\n",
    "    return coordLs, coordRs, coordPs, labels, checks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955404c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# data_dir = 'signData'\n",
    "\n",
    "# train_dir = os.path.join(data_dir,'train')\n",
    "# val_dir = os.path.join(data_dir,'valid')\n",
    "\n",
    "# output_dir = os.path.join(data_dir,'nptxt')\n",
    "# val_output_dir = os.path.join(data_dir,\"nptxt_val\")\n",
    "# weight_dir = os.path.join(data_dir,'weights')\n",
    "\n",
    "# train_landmark_dir = os.path.join(train_dir,'label','landmark')\n",
    "# train_morpheme_dir = os.path.join(train_dir,'label','morpheme')\n",
    "\n",
    "# val_landmark_dir = os.path.join(val_dir,'landmark')\n",
    "# val_morpheme_dir = os.path.join(val_dir,'morpheme')\n",
    "\n",
    "\n",
    "# def getoutputdir(type=\"train\"):\n",
    "#     return val_output_dir if type=='val' else output_dir\n",
    "\n",
    "# with open('wordtonum.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "#     words_dicts = json.load(json_file)\n",
    "\n",
    "\n",
    "# def load_data(file_name,type=\"train\"):\n",
    "#     path=f\"{getoutputdir(type)}/{file_name}\"\n",
    "#     w_path=f\"{weight_dir}/{file_name}\"\n",
    "#     data = np.load(path)\n",
    "#     weight_data = np.load(w_path)\n",
    "#     #좌표값 로드\n",
    "#     wordCoordL = data['wordCoordL']\n",
    "#     wordCoordR = data['wordCoordR']\n",
    "#     wordCoordP = data['wordCoordP']\n",
    "#     # weight = data['weight']\n",
    "#     weight = weight_data['weight']\n",
    "#     #단어 뜻 호출\n",
    "#     ans = data['label'][2]\n",
    "#     ans = ans.replace('\\n', '')\n",
    "#     #해당 단어의 value 호출\n",
    "#     if ans[-1].isdigit():\n",
    "#         ans = ans[:-1]\n",
    "#     label = words_dicts[ans]\n",
    "#     if type=='train':\n",
    "#         return wordCoordL, wordCoordR, wordCoordP, label ,weight\n",
    "#     else:\n",
    "#         return wordCoordL, wordCoordR, wordCoordP, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_word(person, start, num):\n",
    "#     coordLs = []\n",
    "#     coordRs = []\n",
    "#     coordPs = []\n",
    "#     labels = []\n",
    "#     checks = []\n",
    "#     for WNum in range(start, start + num):\n",
    "#         wordCoordL, wordCoordR, wordCoordP, label = load_data(person, WNum)\n",
    "#         coordLs.append(wordCoordL)\n",
    "#         coordRs.append(wordCoordR)\n",
    "#         coordPs.append(wordCoordP)\n",
    "#         labels.append(int(label[0]))\n",
    "#         checks.append((label[1:3]))\n",
    "\n",
    "#     return coordLs, coordRs, coordPs, labels, checks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84c62319e509a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCoordL, wordCoordR, wordCoordP, label ,wordWeight=load_data(\"1/0001.npz\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1425f-71ff-4fec-9b65-253e22e36d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordWeight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_korean(n):\n",
    "    units = ['', '만', '억', '조', '경']\n",
    "    nums = ['영', '일', '이', '삼', '사', '오', '육', '칠', '팔', '구']\n",
    "    small_units = ['', '십', '백', '천']\n",
    "\n",
    "    def split_by_thousands(num):\n",
    "        chunks = []\n",
    "        while num > 0:\n",
    "            chunks.append(num % 10000)\n",
    "            num //= 10000\n",
    "        return chunks\n",
    "\n",
    "    def convert_chunk(chunk):\n",
    "        result = []\n",
    "        for i, digit in enumerate(str(chunk).zfill(4)):\n",
    "            if digit != '0':\n",
    "                result.append(nums[int(digit)] + small_units[3 - i])\n",
    "        return ''.join(result)\n",
    "\n",
    "    if n == 0:\n",
    "        return nums[0]\n",
    "\n",
    "    chunks = split_by_thousands(n)\n",
    "    korean_number = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk > 0:\n",
    "            korean_number.append(convert_chunk(chunk) + units[i])\n",
    "\n",
    "    result = ''.join(reversed(korean_number))\n",
    "    result = result.replace(\"일천\", \"천\").replace(\"일만\", \"만\").replace(\"일십\",\"십\").replace(\"일백\",\"백\")\n",
    "    return result\n",
    "# numbers_01=[number_to_korean(n) for n in range(11,210)]\n",
    "# numbers_02=[number_to_korean(n) for n in range(210,400,10)]\n",
    "# numbers_04=[number_to_korean(n) for n in range(400,1000,50)]\n",
    "# numbers_05=[number_to_korean(n) for n in range(1000,10000,500)]\n",
    "# numbers_06=[number_to_korean(n) for n in range(10000,100000,5000)]\n",
    "# numbers_07=[number_to_korean(n) for n in range(100000,1000000,100000)]\n",
    "# numbers_08=[number_to_korean(n) for n in range(1000000,10000000,1000000)]\n",
    "# numbers_09=[number_to_korean(n) for n in range(10000000,100000000,10000000)]\n",
    "# numbers_10=[number_to_korean(n) for n in range(100000000,1000000000,50000000)]\n",
    "# numbers_11=[number_to_korean(n) for n in range(1000000000,10000000000,500000000)]\n",
    "# numbers_12=[number_to_korean(n) for n in range(10000000000,100000000000,1000000000)]\n",
    "ranges = [\n",
    "    range(11, 210),\n",
    "    range(210, 400, 10),\n",
    "    range(400, 1000, 50),\n",
    "    range(1000, 10000, 500),\n",
    "    range(10000, 100000, 5000),\n",
    "    range(100000, 1000000, 50000),\n",
    "    range(1000000, 10000000, 500000),\n",
    "    range(10000000, 100000000, 5000000),\n",
    "    range(100000000, 1000000000, 50000000),\n",
    "    range(1000000000, 10000000000, 500000000),\n",
    "    range(10000000000, 100000000000, 10000000000),\n",
    "]\n",
    "\n",
    "numbers = [number_to_korean(n) for r in ranges for n in r]\n",
    "numbers.append(\"천억\")\n",
    "numbers.append(\"일조\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_names=[\"지부\",\"센터\",\"협회\",\"지회\",\"시청\",\"특별법\",\"경기대회\",'대학교','도청','청장','0만원']\n",
    "\n",
    "def black_list(name):\n",
    "    if name == \"시청\":\n",
    "        True\n",
    "    elif name in numbers:\n",
    "        return True\n",
    "    else:\n",
    "        for b in black_names:\n",
    "            if b in name:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f6101b194b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make lable\n",
    "\n",
    "\n",
    "\n",
    "person_blacklist=[]\n",
    "word_dict={}\n",
    "num=1\n",
    "for person in os.listdir(train_morpheme_dir):\n",
    "    for word_morpheme in os.listdir(os.path.join(train_morpheme_dir, person)):\n",
    "        if \"F\" in word_morpheme:\n",
    "            file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "            morpheme_file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "            with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "                data = json.load(morpheme_file)\n",
    "                try:\n",
    "                    name = data['data'][0]['attributes'][0]['name']\n",
    "                    name = name.replace(\"\\n\", \"\")\n",
    "                    if name[-1].isdigit():\n",
    "                        name = name[:-1]\n",
    "                    if name in word_dict or black_list(name) or name=='.절':\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(name)\n",
    "                        word_dict[name]=num\n",
    "                        num+=1\n",
    "                except IndexError as e:\n",
    "                    name=False\n",
    "                    continue\n",
    "               \n",
    "    with open('wordtonum_lite.json', 'w') as json_file:\n",
    "        json.dump(word_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66868c66cda60d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879\n"
     ]
    }
   ],
   "source": [
    "with open('wordtonum_lite.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(len(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d944af530fdc7412",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.절'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordtonum_lite.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.절\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '.절'"
     ]
    }
   ],
   "source": [
    "with open('wordtonum_lite.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data['.절'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192b4bd-53f5-4e64-a336-4504f435435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "destination_folder = os.path.join(data_dir, \"nptxt_lite_val\")\n",
    "\n",
    "# JSON 파일 로드\n",
    "with open('wordtonum_lite.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    word_list = json.load(json_file)\n",
    "\n",
    "# 대상 폴더가 없으면 생성\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# 폴더 내 파일 확인 및 복사\n",
    "for person in os.listdir(val_output_dir):\n",
    "    for filename in sorted(os.listdir(os.path.join(val_output_dir, person))):\n",
    "        if filename[0]==\".\":\n",
    "            continue\n",
    "        source_file = os.path.join(person, filename)\n",
    "        print(source_file)\n",
    "        \n",
    "        # 파일 정보 읽기\n",
    "        _, _, _, word = load_data(source_file,\"val\")\n",
    "        print(word)\n",
    "        \n",
    "        # 단어가 리스트에 있으면 복사\n",
    "        if word in word_list:\n",
    "            copy_path = os.path.join(destination_folder, person)\n",
    "            os.makedirs(copy_path, exist_ok=True)\n",
    "            shutil.copy2(os.path.join(val_output_dir, source_file), copy_path)  # 파일 복사\n",
    "            print(f\"Copied: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1648b-4511-4261-9c28-2d806bf7782e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
