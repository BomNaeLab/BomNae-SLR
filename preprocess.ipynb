{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4edd290bccd2f228",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "data_dir = 'signData'\n",
    "\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "val_dir = os.path.join(data_dir,'valid')\n",
    "\n",
    "output_dir = os.path.join(data_dir,'nptxt_CL')\n",
    "weight_dir = os.path.join(data_dir,'weights')\n",
    "val_output_dir = os.path.join(data_dir,\"nptxt_CL_val\")\n",
    "\n",
    "train_landmark_dir = os.path.join(train_dir,'label','landmark')\n",
    "train_morpheme_dir = os.path.join(train_dir,'label','morpheme')\n",
    "\n",
    "val_landmark_dir = os.path.join(val_dir,'landmark')\n",
    "val_morpheme_dir = os.path.join(val_dir,'morpheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7463029f1b6f6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_calc(F, a, b, x):\n",
    "    fps=30\n",
    "    a_frame = a * fps  # a를 프레임 단위로 변환\n",
    "    b_frame = b * fps  # b를 프레임 단위로 변환\n",
    "    x_frame = x * fps  # x(영상의 길이)를 프레임 단위로 변환\n",
    "    if F < a_frame:\n",
    "        return 0\n",
    "    elif a_frame <= F < a_frame + 2 * fps:\n",
    "        return (F - a_frame) / (2 * fps)\n",
    "    elif a_frame + 2 * fps <= F < b_frame:\n",
    "        return 1\n",
    "    elif b_frame <= F <= x_frame:\n",
    "        return 1 - ((F - b_frame) / (x_frame - b_frame)) * 0.5\n",
    "    else:\n",
    "        return 0.5\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f255a616e3863c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train weight data\n",
    "# start_person=\"01\"\n",
    "# start_word=\"0001\"\n",
    "# #사람별 데이터(01~16)\n",
    "# for person in os.listdir(train_landmark_dir):\n",
    "#     #중간부터 다시 데이터 변환시 체크포인트\n",
    "#     if int(person) < int(start_person):\n",
    "#         continue\n",
    "        \n",
    "#     #변환 데이터 저장시 사람 폴더 생성(01~16)\n",
    "#     person_output_path = os.path.join(weight_dir, str(int(person)))\n",
    "#     os.makedirs(person_output_path, exist_ok=True)\n",
    "    \n",
    "#     #각 단어별 좌표값 및 단어 뜻 순회. \n",
    "#     for word_coords, word_morpheme in zip(os.listdir(os.path.join(train_landmark_dir, person)), os.listdir(os.path.join(train_morpheme_dir, person))):\n",
    "#         #중간부터 다시 데이터 변환시 체크 포인트\n",
    "#         if int(word_coords[11:15]) < int(start_word):\n",
    "#             continue\n",
    "#         #정면카메라 외 다른 방향 카메라도 3d좌표값은 동일하여 한개의 방향에서만 데이터 추출\n",
    "#         #단어 별 뜻, 단어의 할당 넘버, 영상 길이와 영상 내 수어 구간 추출\n",
    "#         if \"F\" in word_morpheme:\n",
    "#             file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "#             morpheme_file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "#             with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "#                 data = json.load(morpheme_file)\n",
    "#                 try:\n",
    "#                     num = data['metaData']['name'][11:15]\n",
    "#                     start_sign = data[\"data\"][0][\"start\"]\n",
    "#                     end_sign = data[\"data\"][0][\"end\"]\n",
    "#                     duration = data[\"metaData\"][\"duration\"]\n",
    "#                 #결측치 오류 검출   \n",
    "#                 except IndexError as e:\n",
    "#                     name = False\n",
    "#                     print(f\"Error reading {morpheme_file_path}: {e}\")\n",
    "#                     continue\n",
    "        \n",
    "#         #각 단어 별 좌표 순회시, 단어 뜻이 결측치인 데이터는 제외\n",
    "#         if \"F\" in word_coords and name is not False:\n",
    "#             wordWeight = []\n",
    "#             #단어 영상의 각 프레임별 좌표값 추출\n",
    "#             for frame in os.listdir(os.path.join(train_landmark_dir, person, word_coords)):\n",
    "#                 frame_num = int(frame[25:37])\n",
    "#                 # 가중치 계산만 수행\n",
    "#                 wordWeight.append(weight_calc(frame_num, start_sign, end_sign, duration))\n",
    "            \n",
    "#             # 결과를 저장\n",
    "#             time = [start_sign, end_sign, duration]\n",
    "#             word_output_path = os.path.join(person_output_path, f'{num}.npz')\n",
    "#             np.savez(word_output_path, weight=wordWeight, time=time)\n",
    "#             print(f\"Saved weights for {word_output_path}\")\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131e83bda6105c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "def pre_data(landmark_dir,morpheme_dir,output_dir):\n",
    "        import numpy as np\n",
    "        start_person=\"01\"\n",
    "        start_word=0\n",
    "        #사람별 데이터(01~16)\n",
    "        for person in sorted(os.listdir(landmark_dir)):\n",
    "            #중간부터 다시 데이터 변환시 체크포인트\n",
    "            if int(person) < int(start_person):\n",
    "                continue\n",
    "                \n",
    "            #변환 데이터 저장시 사람 폴더 생성(01~16)\n",
    "            person_output_path = os.path.join(output_dir, str(int(person)))\n",
    "            os.makedirs(person_output_path, exist_ok=True)\n",
    "            \n",
    "            #각 단어별 좌표값 및 단어 뜻 순회. \n",
    "            for word_coords,word_morpheme in zip(sorted(os.listdir(os.path.join(landmark_dir, person))),sorted(os.listdir(os.path.join(morpheme_dir, person)))):\n",
    "                #중간부터 다시 데이터 변환시 체크 포인트\n",
    "                if int(word_coords[11:15]) <start_word:\n",
    "                    continue\n",
    "                elif int(word_coords[11:15]) == start_word:\n",
    "                    start_word=0\n",
    "                #정면카메라 외 다른 방향 카메라도 3d좌표값은 동일하여 한개의 방향에서만 데이터 추출\n",
    "                #단어 별 뜻, 단어의 할당 넘버,영상 길이와 영상 내 수어 구간 추출\n",
    "                file_path = os.path.join(morpheme_dir, person, word_morpheme)\n",
    "                morpheme_file_path = os.path.join(morpheme_dir, person, word_morpheme)\n",
    "                with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "                    data = json.load(morpheme_file)\n",
    "                    try:\n",
    "                        name = data['data'][0]['attributes'][0]['name']\n",
    "                        num = data['metaData']['name'][11:15]\n",
    "                        start_sign = data[\"data\"][0][\"start\"]\n",
    "                        end_sign = data[\"data\"][0][\"end\"]\n",
    "                        duration = data[\"metaData\"][\"duration\"]\n",
    "                    #결측치 오류 검출   \n",
    "                    except IndexError as e:\n",
    "                        name=False\n",
    "                        print(f\"Error reading {morpheme_file_path}: {e}\")\n",
    "                        continue\n",
    "                #각 단어 별 좌표 순회시, 단어 뜻이 결측치인 데이터는 제외\n",
    "                if name is not False:\n",
    "                    wordCoordL = np.empty((0, 4, 5, 3))  \n",
    "                    wordCoordR = np.empty((0, 4, 5, 3))\n",
    "                    wordCoordP = np.empty((0, 3, 10))\n",
    "                    wordWeight = []\n",
    "                    #단어 영상의 각 프레임별 좌표값 추출            \n",
    "                    for frame in sorted(os.listdir(os.path.join(landmark_dir, person, word_coords))):\n",
    "                        \n",
    "                        file_path = os.path.join(landmark_dir, person, word_coords, frame)\n",
    "                        frame_num=int(frame[25:37])\n",
    "                        with open(file_path, 'r') as json_file:\n",
    "                            data = json.load(json_file)\n",
    "                            lh_points = data['people']['hand_left_keypoints_3d']\n",
    "                            rh_points = data['people']['hand_right_keypoints_3d']\n",
    "                            p_points = data['people']['pose_keypoints_3d']\n",
    "                            #포즈 좌표값 추출\n",
    "                            #1920x1080영상을 메디어파이프의 특성상 영상의 중심을 기준으로 1080x1080으로 잘라서 기존 죄표를 픽셀 값으로 변환 후 \n",
    "                            # 다시 미디어 파이프와 같은 스케일인 0~1사이 값으로 정규화\n",
    "                            #z좌표는 미디어파이프의 기준이 되는 랜드마크의 좌표 활용하여 계산\n",
    "                            \n",
    "                            preFrameCoordP = np.array([[(960 * p_points[i] + 960), #중앙 랜드마크 x 0.5되도록  \n",
    "                                                        (1080 * p_points[i + 1] + 540),\n",
    "                                                        (p_points[32 + 2] - p_points[i + 2]) / 10]\n",
    "                                                    for i in range(0, len(p_points), 4)], dtype=np.float32)\n",
    "\n",
    "                            preFrameCoordL = np.array([[(960 * lh_points[i] + 960),\n",
    "                                                        (1080 * lh_points[i + 1] + 540),\n",
    "                                                        (lh_points[2] - lh_points[i + 2]) / 10]\n",
    "                                                    for i in range(4, len(lh_points), 4)], dtype=np.float32)\n",
    "\n",
    "                            preFrameCoordR = np.array([[(960 * rh_points[i] + 960),\n",
    "                                                        (1080 * rh_points[i + 1] + 540),\n",
    "                                                        (rh_points[2] - rh_points[i + 2]) / 10]\n",
    "                                                    for i in range(4, len(rh_points), 4)], dtype=np.float32)\n",
    "                            \n",
    "                            \n",
    "                            #포즈 좌표에서 하반신 랜드마크 제외\n",
    "                            preFrameCoordP = preFrameCoordP[[i for i in range(19) if (0 <= i <= 7) or (17 <= i <= 18)]]\n",
    "                            \n",
    "                            #랜드마크 값들을 모델 트레이닝에 적합한 모양으로 재배열.\n",
    "                            frameCoordL=preFrameCoordL.reshape(5,4,3).transpose(1,0,2)[::-1]\n",
    "                            frameCoordR=preFrameCoordR.reshape(5,4,3).transpose(1,0,2)[::-1]\n",
    "                            frameCoordP=preFrameCoordP.T\n",
    "\n",
    "                            import numpy as np\n",
    "\n",
    "\n",
    "                            # 1번 인덱스의 x, y 좌표\n",
    "                            x_center = frameCoordP[0][1]\n",
    "                            y_center = frameCoordP[1][1]\n",
    "\n",
    "                            # 1080x1080 크롭 영역을 정하기 위해 크롭 영역의 좌상단 좌표 계산\n",
    "                            x_min = max(0, x_center - 540)\n",
    "                            y_min = max(0, y_center - 540)\n",
    "\n",
    "                            # 크롭 영역이 1920x1080을 초과하지 않도록 조정\n",
    "                            x_min = min(x_min, 1920 - 1080)\n",
    "                            y_min = min(y_min, 1080 - 1080)\n",
    "\n",
    "                            # 크롭 영역의 우상단 좌표 계산\n",
    "                            x_max = x_min + 1080\n",
    "                            y_max = y_min + 1080\n",
    "\n",
    "                            # 좌표들을 크롭 영역에 맞게 이동\n",
    "                            frameCoordP[0] -= x_min\n",
    "                            frameCoordP[1] -= y_min\n",
    "                            frameCoordP[0] /= 1080\n",
    "                            frameCoordP[1] /= 1080\n",
    "\n",
    "                            frameCoordP[0] = np.clip(frameCoordP[0], 0, 1)\n",
    "                            frameCoordP[1] = np.clip(frameCoordP[1], 0, 1)\n",
    "\n",
    "                            \n",
    "                            def min_cal(coords):\n",
    "                                coord_x_min = np.min(coords[:, :, 0]if coords.ndim == 3 else coords[:, 0])\n",
    "                                coord_x_cal = np.max(coords[:, :, 0]if coords.ndim == 3 else coords[:, 0]) - coord_x_min \n",
    "\n",
    "                                coord_y_min = np.min(coords[:, :, 1]if coords.ndim == 3 else coords[:, 1])\n",
    "                                coord_y_cal = np.max(coords[:, :, 1]if coords.ndim == 3 else coords[:, 1]) - coord_y_min \n",
    "                                \n",
    "                                return [coord_x_min,coord_y_min,np.min(frameCoordL[:, :, 2]if coords.ndim == 3 else coords[:, 2])],[coord_x_cal,coord_y_cal]\n",
    "\n",
    "                            coordL_min,coordL_cal=min_cal(frameCoordL)\n",
    "                            coordR_min,coordR_cal=min_cal(frameCoordR)\n",
    "\n",
    "                            frameCoordL = (frameCoordL - coordL_min) / (coordL_cal[0] if coordL_cal[0] > coordL_cal[1] else coordL_cal[1])\n",
    "                            frameCoordR = (frameCoordR - coordR_min) / (coordR_cal[0] if coordR_cal[0] > coordR_cal[1] else coordR_cal[1])\n",
    "                            \n",
    "                            #한 단어의 프레임 별 좌표를 하나의 넘파이 배열로 저장\n",
    "                            wordCoordL = np.append(wordCoordL, [frameCoordL], axis=0)\n",
    "                            wordCoordR = np.append(wordCoordR, [frameCoordR], axis=0)\n",
    "                            wordCoordP = np.append(wordCoordP, [frameCoordP], axis=0)\n",
    "                            wordWeight.append(weight_calc(frame_num,start_sign,end_sign,duration))\n",
    "                            \n",
    "                    label=[name,num,start_sign,end_sign,duration]\n",
    "                    word_output_path = os.path.join(person_output_path, f'{num}.npz')\n",
    "                    np.savez(word_output_path, wordCoordL=wordCoordL, wordCoordR=wordCoordR, wordCoordP=wordCoordP, label=label, weight=wordWeight)\n",
    "                    print(f\"Saved {word_output_path}\")\n",
    "\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b458f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data(val_landmark_dir,val_morpheme_dir,val_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(frameCoordP[0], frameCoordP[1], color='blue', label=\"Transformed Points\")\n",
    "plt.scatter([frameCoordP[0][1]], [frameCoordP[1][1]], color='red', label=\"Center Point (Index 1)\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(0.5, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0.5, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.title(\"Normalized Coordinates after Cropping and Centering\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c517327bb530d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #val data\n",
    "# start_person=\"01\"\n",
    "# start_word=\"0001\"\n",
    "# #사람별 데이터(01~16)\n",
    "# for person in os.listdir(val_landmark_dir):  # train_landmark_dir -> val_landmark_dir\n",
    "#     #중간부터 다시 데이터 변환시 체크포인트\n",
    "#     if int(person) < int(start_person):\n",
    "#         continue\n",
    "        \n",
    "#     #변환 데이터 저장시 사람 폴더 생성(01~16)\n",
    "#     person_output_path = os.path.join(val_output_dir, str(int(person)))\n",
    "#     os.makedirs(person_output_path, exist_ok=True)\n",
    "    \n",
    "#     #각 단어별 좌표값 및 단어 뜻 순회. \n",
    "#     for word_coords,word_morpheme in zip(os.listdir(os.path.join(val_landmark_dir, person)),os.listdir(os.path.join(val_morpheme_dir, person))):  # train_landmark_dir -> val_landmark_dir, train_morpheme_dir -> val_morpheme_dir\n",
    "#         #중간부터 다시 데이터 변환시 체크 포인트\n",
    "#         if int(word_coords[11:15]) < int(start_word):\n",
    "#             continue\n",
    "#         #정면카메라 외 다른 방향 카메라도 3d좌표값은 동일하여 한개의 방향에서만 데이터 추출\n",
    "#         #단어 별 뜻, 단어의 할당 넘버,영상 길이와 영상 내 수어 구간 추출\n",
    "#         if \"F\" in word_morpheme:\n",
    "#             file_path = os.path.join(val_morpheme_dir, person, word_morpheme)  # train_morpheme_dir -> val_morpheme_dir\n",
    "#             morpheme_file_path = os.path.join(val_morpheme_dir, person, word_morpheme)  # train_morpheme_dir -> val_morpheme_dir\n",
    "#             with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "#                 data = json.load(morpheme_file)\n",
    "#                 try:\n",
    "#                     name = data['data'][0]['attributes'][0]['name'].replace('\\n', '')\n",
    "#                     num = data['metaData']['name'][11:15]\n",
    "#                 #결측치 오류 검출   \n",
    "#                 except IndexError as e:\n",
    "#                     name = False\n",
    "#                     print(f\"Error reading {morpheme_file_path}: {e}\")\n",
    "#                     continue\n",
    "#         #각 단어 별 좌표 순회시, 단어 뜻이 결측치인 데이터는 제외\n",
    "#         if \"F\" in word_coords and name is not False:\n",
    "#             wordCoordL = np.empty((0, 4, 5, 3))  \n",
    "#             wordCoordR = np.empty((0, 4, 5, 3))\n",
    "#             wordCoordP = np.empty((0, 3, 10))\n",
    "#             wordWeight = []\n",
    "#             #단어 영상의 각 프레임별 좌표값 추출            \n",
    "#             for frame in os.listdir(os.path.join(val_landmark_dir, person, word_coords)):  # train_landmark_dir -> val_landmark_dir\n",
    "#                 file_path = os.path.join(val_landmark_dir, person, word_coords, frame)  # train_landmark_dir -> val_landmark_dir\n",
    "#                 frame_num = int(frame[25:37])\n",
    "#                 with open(file_path, 'r') as json_file:\n",
    "#                     data = json.load(json_file)\n",
    "#                     lh_points = data['people']['hand_left_keypoints_3d']\n",
    "#                     rh_points = data['people']['hand_right_keypoints_3d']\n",
    "#                     p_points = data['people']['pose_keypoints_3d']\n",
    "#                     #포즈 좌표값 추출 및 정규화 코드 동일\n",
    "#                     preFrameCoordP = np.array([[(960 * p_points[i] + 960 - 420) / (1500 - 420), \n",
    "#                                                 (1080 * p_points[i + 1] + 540) / 1080,\n",
    "#                                                 (p_points[32 + 2] - p_points[i + 2]) / 10]\n",
    "#                                                for i in range(0, len(p_points), 4)], dtype=np.float32)\n",
    "\n",
    "#                     preFrameCoordL = np.array([[(960 * lh_points[i] + 960 - 420) / (1500 - 420),\n",
    "#                                                 (1080 * lh_points[i + 1] + 540) / 1080,\n",
    "#                                                 (lh_points[2] - lh_points[i + 2]) / 10]\n",
    "#                                                for i in range(4, len(lh_points), 4)], dtype=np.float32)\n",
    "\n",
    "#                     preFrameCoordR = np.array([[(960 * rh_points[i] + 960 - 420) / (1500 - 420),\n",
    "#                                                 (1080 * rh_points[i + 1] + 540) / 1080,\n",
    "#                                                 (rh_points[2] - rh_points[i + 2]) / 10]\n",
    "#                                                for i in range(4, len(rh_points), 4)], dtype=np.float32)\n",
    "                    \n",
    "#                     #포즈 좌표에서 하반신 랜드마크 제외\n",
    "#                     preFrameCoordP = preFrameCoordP[[i for i in range(19) if (0 <= i <= 7) or (17 <= i <= 18)]]\n",
    "                    \n",
    "#                     #랜드마크 값들을 모델 트레이닝에 적합한 모양으로 재배열.\n",
    "#                     frameCoordL = preFrameCoordL.reshape(5, 4, 3).transpose(1, 0, 2)[::-1]\n",
    "#                     frameCoordR = preFrameCoordR.reshape(5, 4, 3).transpose(1, 0, 2)[::-1]\n",
    "#                     frameCoordP = preFrameCoordP.T\n",
    "#                     #한 단어의 프레임 별 좌표를 하나의 넘파이 배열로 저장\n",
    "#                     wordCoordL = np.append(wordCoordL, [frameCoordL], axis=0)\n",
    "#                     wordCoordR = np.append(wordCoordR, [frameCoordR], axis=0)\n",
    "#                     wordCoordP = np.append(wordCoordP, [frameCoordP], axis=0)\n",
    "\n",
    "                    \n",
    "#             label = [int(person),num,name]\n",
    "#             word_output_path = os.path.join(person_output_path, f'{num}.npz')\n",
    "#             np.savez(word_output_path, wordCoordL=wordCoordL, wordCoordR=wordCoordR, wordCoordP=wordCoordP, label=label)\n",
    "#             print(f\"Saved {word_output_path}\")\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d38785c53bd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'signData'\n",
    "\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "val_dir = os.path.join(data_dir,'valid')\n",
    "\n",
    "output_dir = os.path.join(data_dir,'nptxt_CL')\n",
    "val_output_dir = os.path.join(data_dir,\"nptxt_CL_val\")\n",
    "# weight_dir = os.path.join(data_dir,'weights')\n",
    "\n",
    "train_landmark_dir = os.path.join(train_dir,'label','landmark')\n",
    "train_morpheme_dir = os.path.join(train_dir,'label','morpheme')\n",
    "\n",
    "val_landmark_dir = os.path.join(val_dir,'landmark')\n",
    "val_morpheme_dir = os.path.join(val_dir,'morpheme')\n",
    "\n",
    "\n",
    "def getoutputdir(type=\"train\"):\n",
    "    return val_output_dir if type=='val' else output_dir\n",
    "\n",
    "with open('wordtonum.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    words_dicts = json.load(json_file)\n",
    "\n",
    "def load_data(file_name,type=\"train\"):\n",
    "    path=f\"{getoutputdir(type)}/{file_name}\"\n",
    "\n",
    "    data = np.load(path)\n",
    "\n",
    "    #좌표값 로드\n",
    "    wordCoordL = data['wordCoordL']\n",
    "    wordCoordR = data['wordCoordR']\n",
    "    wordCoordP = data['wordCoordP']\n",
    "    wordWeight = data['weight']\n",
    "    #단어 뜻 호출\n",
    "    ans = data['label'][0]\n",
    "    ans = ans.replace('\\n', '')\n",
    "    #해당 단어의 value 호출\n",
    "    if ans[-1].isdigit():\n",
    "        ans = ans[:-1]\n",
    "    label = words_dicts[ans]\n",
    "    if type=='train':\n",
    "        return wordCoordL, wordCoordR, wordCoordP, label ,wordWeight\n",
    "    else:\n",
    "        return wordCoordL, wordCoordR, wordCoordP, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_word(person, start, num):\n",
    "    coordLs = []\n",
    "    coordRs = []\n",
    "    coordPs = []\n",
    "    labels = []\n",
    "    checks = []\n",
    "    for WNum in range(start, start + num):\n",
    "        wordCoordL, wordCoordR, wordCoordP, label = load_data(person, WNum)\n",
    "        coordLs.append(wordCoordL)\n",
    "        coordRs.append(wordCoordR)\n",
    "        coordPs.append(wordCoordP)\n",
    "        labels.append(int(label[0]))\n",
    "        checks.append((label[1:3]))\n",
    "\n",
    "    return coordLs, coordRs, coordPs, labels, checks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c84c62319e509a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCoordL, wordCoordR, wordCoordP, label ,wordWeight=load_data(\"1/0001.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e1425f-71ff-4fec-9b65-253e22e36d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordWeight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2b903-11ec-40ac-89f1-e3dd5eb1e0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541f6101b194b2a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m word_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m      7\u001b[0m num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m person \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(train_morpheme_dir):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word_morpheme \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_morpheme_dir, person)):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m word_morpheme:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#make lable\n",
    "\n",
    "\n",
    "\n",
    "person_blacklist=[]\n",
    "word_dict={}\n",
    "num=1\n",
    "for person in os.listdir(train_morpheme_dir):\n",
    "    for word_morpheme in os.listdir(os.path.join(train_morpheme_dir, person)):\n",
    "        if \"F\" in word_morpheme:\n",
    "            file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "            morpheme_file_path = os.path.join(train_morpheme_dir, person, word_morpheme)\n",
    "            with open(morpheme_file_path, 'r', encoding=\"UTF8\") as morpheme_file:\n",
    "                data = json.load(morpheme_file)\n",
    "                try:\n",
    "                    name = data['data'][0]['attributes'][0]['name']\n",
    "                    name = name.replace(\"\\n\", \"\")\n",
    "                    if name[-1].isdigit():\n",
    "                        name = name[:-1]\n",
    "                    if name in word_dict:\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(name)\n",
    "                        word_dict[name]=num\n",
    "                        num+=1\n",
    "                except IndexError as e:\n",
    "                    name=False\n",
    "                    continue\n",
    "               \n",
    "    with open('wordtonum.json', 'w') as json_file:\n",
    "        json.dump(word_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66868c66cda60d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordtonum.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944af530fdc7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordtonum.json', 'r', encoding=\"UTF8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data['신부'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
