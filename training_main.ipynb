{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d46b6480881db",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import preprocess as prep\n",
    "import SLR_model\n",
    "import SLR_model_GRU\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "import preprocess\n",
    "# from preprocess import person\n",
    "\n",
    "\n",
    "\n",
    "# model will output multiple(5) results/sec, how are we gonna handle it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8634f5da68e224",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "load_size = 3000 # number of data to be loaded at once\n",
    "epochs = 50\n",
    "run_time=2\n",
    "batch_size = 16\n",
    "save_dir = \"saves_GRU\"\n",
    "load_dir = \"saves_GRU\"\n",
    "model = SLR_model_GRU.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5961b31976929d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:23:21.058102Z",
     "start_time": "2024-11-02T13:23:18.527158Z"
    }
   },
   "outputs": [],
   "source": [
    " # reload model file\n",
    "end_file=preprocess.getoutputdir()\n",
    "\n",
    "save_suffix = time.strftime(\"%m-%d-%H\", time.localtime(time.time()))\n",
    "ckpt_name=save_suffix+\"-\"+str(epochs)+\"epochs-\"+str(run_time)+\"times\"\n",
    "\n",
    "check_path = os.path.join(save_dir,'ckpt',ckpt_name)\n",
    "hist_path = os.path.join(save_dir, \"hist\",ckpt_name+\".json\")\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "filepath=f'{check_path}.keras',\n",
    "monitor='categorical_accuracy',\n",
    "mode='max',\n",
    "save_freq='epoch',\n",
    "save_best_only=True)\n",
    "\n",
    "\n",
    "start_person=1\n",
    "start_count=1\n",
    "start_word=\" \"\n",
    "with open(os.path.join('logs',ckpt_name+'.txt'), 'a') as logs:\n",
    "    for i in range(1,17):\n",
    "        l_raws=[]\n",
    "        r_raws=[]\n",
    "        p_raws=[]\n",
    "        y_raws=[]\n",
    "        loss_weights_raws=[]\n",
    "        if start_person>i:\n",
    "            continue\n",
    "        for k in range(1,run_time+1):\n",
    "            if start_count>k:\n",
    "                continue\n",
    "            elif start_count==k:\n",
    "                start_count=0\n",
    "            for j in sorted(os.listdir(os.path.join(preprocess.getoutputdir(),str(i)))):\n",
    "                if len(l_raws)==0:\n",
    "                    start_word = j\n",
    "                else:\n",
    "                    end_word = j\n",
    "                l_raw, r_raw, p_raw, y_raw, loss_weights_raw = preprocess.load_data(f\"{i}/{j}\")\n",
    "                l_raws.append(l_raw)\n",
    "                r_raws.append(r_raw)\n",
    "                p_raws.append(p_raw)\n",
    "                y_raws.append(y_raw)\n",
    "                loss_weights_raws.append(loss_weights_raw)\n",
    "                # print(p_raw.shape)\n",
    "                # break\n",
    "                if len(l_raws)>=load_size:\n",
    "                    \n",
    "                    logs.write(f'{ time.strftime(\"%H-%M-%S\", time.localtime(time.time()))}:{k}) person:{i} : {start_word} ~ {end_word}\\n')  # 한 줄 쓰기\n",
    "                    l_train, each = SLR_model_GRU.serialize(l_raws)\n",
    "                    r_train, each = SLR_model_GRU.serialize(r_raws)\n",
    "                    p_train, each, sample_weights = SLR_model_GRU.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                    x_train = (l_train, r_train, p_train)\n",
    "                    \n",
    "                    y_train = np.repeat(y_raws, each)\n",
    "                    y_train = SLR_model_GRU.encode_onehot2d(y_train)\n",
    "                    \n",
    "                    dataset = SLR_model_GRU.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                    hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                    \n",
    "                    # model.save(os.path.join(save_dir, 'model', f\"{ckpt_name}.h5\"))\n",
    "\n",
    "                    # converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "                    # tflite_model = converter.convert()\n",
    "                    # with open(os.path.join(save_dir, 'model', f\"{ckpt_name}.tflite\"), 'wb') as tflite_file:\n",
    "                    #     tflite_file.write(tflite_model)\n",
    "\n",
    "                    with open(hist_path, 'w') as file:\n",
    "                        json.dump(hist.history, file)\n",
    "                    l_raws.clear()\n",
    "                    r_raws.clear()\n",
    "                    p_raws.clear()\n",
    "                    y_raws.clear()\n",
    "                    loss_weights_raws.clear()\n",
    "            if len(l_raws)>0:\n",
    "                l_train, each = SLR_model_GRU.serialize(l_raws)\n",
    "                r_train, each = SLR_model_GRU.serialize(r_raws)\n",
    "                p_train, each, sample_weights = SLR_model_GRU.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                x_train = (l_train, r_train, p_train)\n",
    "                \n",
    "                y_train = np.repeat(y_raws, each)\n",
    "                y_train = SLR_model_GRU.encode_onehot2d(y_train)\n",
    "                \n",
    "                \n",
    "                dataset = SLR_model_GRU.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                \n",
    "                # model.save(os.path.join(save_dir, 'model', f\"{ckpt_name}.h5\"))\n",
    "\n",
    "                # converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "                # tflite_model = converter.convert()\n",
    "                # with open(os.path.join(save_dir, 'model', f\"{ckpt_name}.tflite\"), 'wb') as tflite_file:\n",
    "                #     tflite_file.write(tflite_model)\n",
    "\n",
    "                with open(hist_path, 'w') as file:\n",
    "                    json.dump(hist.history, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e74e08d3b98fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "load_size = 3000 # number of data to be loaded at once\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "save_dir = \"saves\"\n",
    "load_dir = \"saves\"\n",
    "# load_path = \"C:/Users/jerry/Desktop/hly/2024-2/cap/BN_SLR/BomNae-SLR/checkpoints.keras\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a3ca3f4b1820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SLR_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb6cd65c6b35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # reload model file\n",
    "importlib.reload(SLR_model)\n",
    "importlib.reload(preprocess)\n",
    "end_file=preprocess.getoutputdir()\n",
    "\n",
    "save_suffix = time.strftime(\"_%d-%H-%M\", time.localtime(time.time()))\n",
    "\n",
    "\n",
    "# checkTime=\"_14-16-16\"\n",
    "# check_path = os.path.join(save_dir, \"check\"+checkTime)\n",
    "# hist_path = os.path.join(save_dir, \"hist\"+checkTime+\".json\")\n",
    "\n",
    "check_path = os.path.join(save_dir, \"check\"+save_suffix+\"_50epochs_3time\")\n",
    "hist_path = os.path.join(save_dir, \"hist\"+save_suffix+\".json\")\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "filepath=f'{check_path}.keras',\n",
    "monitor='categorical_accuracy',\n",
    "mode='max',\n",
    "save_freq='epoch',\n",
    "save_best_only=True)\n",
    "\n",
    "\n",
    "start_person=1\n",
    "start_count=1\n",
    "start_word=\" \"\n",
    "for i in range(1,17):\n",
    "    l_raws=[]\n",
    "    r_raws=[]\n",
    "    p_raws=[]\n",
    "    y_raws=[]\n",
    "    loss_weights_raws=[]\n",
    "    if start_person>i:\n",
    "        continue\n",
    "    for k in range(1,2):\n",
    "        if start_count>k:\n",
    "            continue\n",
    "        elif start_count==k:\n",
    "            start_count=0\n",
    "        for j in sorted(os.listdir(os.path.join(preprocess.getoutputdir(),str(i)))):\n",
    "            if len(l_raws)==0:\n",
    "                start_word = j\n",
    "            else:\n",
    "                end_word = j\n",
    "            l_raw, r_raw, p_raw, y_raw, loss_weights_raw = preprocess.load_data(f\"{i}/{j}\")\n",
    "            l_raws.append(l_raw)\n",
    "            r_raws.append(r_raw)\n",
    "            p_raws.append(p_raw)\n",
    "            y_raws.append(y_raw)\n",
    "            loss_weights_raws.append(loss_weights_raw)\n",
    "    \n",
    "            if len(l_raws)>=load_size:\n",
    "                with open(str(i)+'training_log.txt', 'a') as logs:\n",
    "                    logs.write(f'{ time.strftime(\"%H-%M-%S\", time.localtime(time.time()))}:{k}) person:{i} : {start_word} ~ {end_word}\\n')  # 한 줄 쓰기\n",
    "                print(f\"{k}) person:{i} : {start_word} ~ {end_word}\\n\\n\\n\\n\")\n",
    "                l_train, each = SLR_model.serialize(l_raws)\n",
    "                r_train, each = SLR_model.serialize(r_raws)\n",
    "                p_train, each, sample_weights = SLR_model.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                x_train = (l_train, r_train, p_train)\n",
    "                \n",
    "                y_train = np.repeat(y_raws, each)\n",
    "                y_train = SLR_model.encode_onehot2d(y_train)\n",
    "    \n",
    "                dataset = SLR_model.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                \n",
    "                with open(hist_path, 'w') as file:\n",
    "                    json.dump(hist.history, file)\n",
    "                l_raws.clear()\n",
    "                r_raws.clear()\n",
    "                p_raws.clear()\n",
    "                y_raws.clear()\n",
    "                loss_weights_raws.clear()\n",
    "        if len(l_raws)>0:\n",
    "            l_train, each = SLR_model.serialize(l_raws)\n",
    "            r_train, each = SLR_model.serialize(r_raws)\n",
    "            p_train, each, sample_weights = SLR_model.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "            x_train = (l_train, r_train, p_train)\n",
    "            \n",
    "            y_train = np.repeat(y_raws, each)\n",
    "            y_train = SLR_model.encode_onehot2d(y_train)\n",
    "    \n",
    "            dataset = SLR_model.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "            hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "            \n",
    "            with open(hist_path, 'w') as file:\n",
    "                json.dump(hist.history, file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501d29fe7faee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand input shape: (batch, time, h, w, channels)\n",
    "# pose input shape: (batch, time, channel, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96948ee065658f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 세이브된 모델 로드\n",
    "# load_path = os.join(load_dir, \"check_00-00-00.keras\")\n",
    "# model = SLR_model.load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac698d86f47869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 안될때 (강제중지 + 초기화)\n",
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755ee45fa0f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_raws=[]\n",
    "# r_raws=[]\n",
    "# p_raws=[]\n",
    "# y_raws=[]\n",
    "\n",
    "\n",
    "# save_suffix = time.strftime(\"_%d-%H-%M\", time.localtime(time.time()))\n",
    "# check_path = os.path.join(save_dir, \"check\"+save_suffix)\n",
    "# hist_path = os.path.join(save_dir, \"hist\"+save_suffix+\".json\")\n",
    "\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=f'{check_path}.keras',\n",
    "#     monitor='binary_accuracy',\n",
    "#     mode='max',\n",
    "#     save_freq='epoch',\n",
    "#     save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "# l_raw, r_raw, p_raw, y_raw = preprocess.load_data(f\"1/1\")\n",
    "\n",
    "# l_raws.append(l_raw)\n",
    "# r_raws.append(r_raw)\n",
    "# p_raws.append(p_raw)\n",
    "# y_raws.append(y_raw)\n",
    "\n",
    "\n",
    "# l_train, each = SLR_model.serialize(l_raws)\n",
    "# r_train, each = SLR_model.serialize(r_raws)\n",
    "# p_train, each = SLR_model.serialize(p_raws, stride=2)\n",
    "# y_train = np.repeat(y_raws, each)\n",
    "# y_train = SLR_model.num_arr2bin(y_train, 12)\n",
    "# x_train = (l_train, r_train, p_train)\n",
    "\n",
    "# # dataset = SLR_model.convert_to_dataset(x_train, y_train, batch_size)\n",
    "# hist = model.fit(x_train, y_train, batch_size = 2, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# with open(hist_path, 'w') as file:\n",
    "#     json.dump(hist.history, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bad08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(hist.history['loss'])), hist.history['loss'])\n",
    "plt.scatter(range(len(hist.history['loss'])), hist.history['loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend([\"loss\"])\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981331a75aa7e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:02:15.876445Z",
     "start_time": "2024-10-14T14:02:15.873228Z"
    }
   },
   "outputs": [],
   "source": [
    "print(os.listdir(os.path.join(preprocess.getoutputdir(),str(15))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd1aad3ed6ed7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BomNae-SLR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
