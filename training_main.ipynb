{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d46b6480881db",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "import preprocess_mjy as preprocess\n",
    "# import preprocess as prep\n",
    "\n",
    "# import SLR_model\n",
    "import SLR_model_CNN_GRU\n",
    "# import SLR_model_CL_GRU\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# model will output multiple(5) results/sec, how are we gonna handle it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8634f5da68e224",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "load_size = 3000 # number of data to be loaded at once\n",
    "epochs = 10\n",
    "run_time= 5\n",
    "# batch_size = 16\n",
    "save_dir = \"saves_CNN_GRU\"\n",
    "load_dir = \"saves_CNN_GRU\"\n",
    "model = SLR_model_CNN_GRU.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9501d29fe7faee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand input shape: (batch, time, h, w, channels)\n",
    "# pose input shape: (batch, time, channel, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886aede8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'SLR_model_CNN_GRU' from 'd:\\\\Users\\\\jerry.DESKTOP-KQESKMB\\\\Desktop\\\\pyt\\\\sign\\\\git_folder\\\\BomNae-SLR\\\\SLR_model_CNN_GRU.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importlib.reload(SLR_model_CNN_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e9db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\jerry.DESKTOP-KQESKMB\\.conda\\envs\\bn_slr\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cngru_model = SLR_model_CNN_GRU.reinit_model(run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82aeb818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.npz\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - categorical_accuracy: 0.0000e+00 - loss: 8.1983\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 5.0815\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 3.8091\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 1.9722\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 1.1890\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.4406\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.1122\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0338\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0119\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0043\n",
      "0002.npz\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 14.2979\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 0.0000e+00 - loss: 13.4975\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - categorical_accuracy: 0.0000e+00 - loss: 12.6786\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 0.0000e+00 - loss: 11.6038\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - categorical_accuracy: 0.0000e+00 - loss: 10.5455\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 0.0000e+00 - loss: 9.3979\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 8.1383\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 6.6369\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 5.2475\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - categorical_accuracy: 0.0000e+00 - loss: 3.8168\n",
      "0003.npz\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 11.1126\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 10.0971\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 9.4365\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 8.9408\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 8.4200\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 7.7651\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 6.9547\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 5.9994\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - categorical_accuracy: 0.0000e+00 - loss: 4.9192\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# print('yt:', y_train.shape)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# print(\"li:\",x_train[0].shape)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# print(\"ri:\",x_train[1].shape)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# dataset = SLR_model_CNN_GRU.convert_to_dataset(x_train, y_train, batch_size= 1)#, sample_weights= sample_weights)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# hist = cngru_model.fit(x_train, y_train, batch_size=1, epochs=epochs)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chkpt_elapsed \u001b[38;5;241m<\u001b[39m load_size \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# train without checkpoint\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# print(cngru_model.summary())\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[43mcngru_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     chkpt_elapsed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# train with checkpoint every {load_size}th video\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\jerry.DESKTOP-KQESKMB\\.conda\\envs\\bn_slr\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Users\\jerry.DESKTOP-KQESKMB\\.conda\\envs\\bn_slr\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:319\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m--> 319\u001b[0m         \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m(step)\n\u001b[0;32m    320\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PER VIDEO TRAINING LOOP\n",
    "\n",
    "end_file=preprocess.getoutputdir()\n",
    "\n",
    "save_suffix = time.strftime(\"%m-%d-%H\", time.localtime(time.time()))\n",
    "ckpt_name=save_suffix+\"-\"+str(epochs)+\"epochs-\"+str(run_time)+\"times\"\n",
    "\n",
    "check_path = os.path.join(save_dir,'ckpt',ckpt_name)\n",
    "hist_path = os.path.join(save_dir, \"hist\",ckpt_name+\".json\")\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "filepath=f'{check_path}.keras',\n",
    "monitor='categorical_accuracy',\n",
    "mode='max',\n",
    "save_freq='epoch',\n",
    "save_best_only=True)\n",
    "\n",
    "\n",
    "start_person=1\n",
    "\n",
    "for i in range(1,17):\n",
    "    if start_person>i:\n",
    "        continue\n",
    "    for k in range(1,run_time+1):\n",
    "        chkpt_elapsed = 0\n",
    "        file_list = sorted(os.listdir(os.path.join(preprocess.getoutputdir(),str(i))))\n",
    "        for j in file_list: # every video\n",
    "            print(j)\n",
    "            l_raw, r_raw, p_raw, y_raw, loss_weights_raw = preprocess.load_data(f\"{i}/{j}\")\n",
    "            with open(os.path.join('logs',ckpt_name+'.txt'), 'a') as logs:\n",
    "                hist = None # declaration for scope\n",
    "                logs.write(f'{ time.strftime(\"%H-%M-%S\", time.localtime(time.time()))}:{k}) person:{i} : {j}\\n')  # 한 줄 쓰기\n",
    "                ## ----------------------------------------------------\n",
    "                ## CLGRU <-> CNNGRU 전환시 serialize 인자값 바꿔줄 것\n",
    "                ## --------------------------------------\n",
    "                # PER VIDEO TRAINING LOOP\n",
    "                l_train, window_count = SLR_model_CNN_GRU.serialize(l_raw, stride=1, window_size=17, hop_length=10)\n",
    "                r_train, window_count = SLR_model_CNN_GRU.serialize(r_raw, stride=1, window_size=17, hop_length=10)\n",
    "                p_train, window_count, sample_weights = SLR_model_CNN_GRU.serialize(p_raw, stride=2, window_size=17, hop_length=10, loss_weights=loss_weights_raw, is_pose=True)\n",
    "                x_train = (l_train, r_train, p_train)\n",
    "\n",
    "                # y_train = np.repeat(y_raw, window_count)\n",
    "                # print(y_raw.shape)\n",
    "                y_train = SLR_model_CNN_GRU.encode_onehot2d(y_raw)\n",
    "                y_train = tf.expand_dims(y_train, axis=0)\n",
    "                # print('yt:', y_train.shape)\n",
    "                # print(\"li:\",x_train[0].shape)\n",
    "                # print(\"ri:\",x_train[1].shape)\n",
    "                # print(x_train[2].shape)\n",
    "                # print(sample_weights.shape)\n",
    "                # dataset = SLR_model_CNN_GRU.convert_to_dataset(x_train, y_train, batch_size= 1)#, sample_weights= sample_weights)\n",
    "                # hist = cngru_model.fit(x_train, y_train, batch_size=1, epochs=epochs)\n",
    "                                \n",
    "                if chkpt_elapsed < load_size and j is not file_list[-1]:\n",
    "                    # train without checkpoint\n",
    "                    # print(cngru_model.summary())\n",
    "                    hist = cngru_model.fit(x_train, y_train, batch_size=1, epochs=epochs)\n",
    "                    chkpt_elapsed += 1\n",
    "                else:\n",
    "                    # train with checkpoint every {load_size}th video\n",
    "                    hist = cngru_model.fit(x_train, y_train, batch_size=1, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                    chkpt_elapsed = 0   \n",
    "\n",
    "                with open(hist_path, 'w') as file:\n",
    "                    json.dump(hist.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5961b31976929d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:59:17.134279Z",
     "start_time": "2024-11-06T11:59:17.102279Z"
    }
   },
   "outputs": [],
   "source": [
    "# LEGACY TRAINING LOOP WITH LOAD SIZE  \n",
    "\n",
    " # reload model file\n",
    "end_file=preprocess.getoutputdir()\n",
    "\n",
    "save_suffix = time.strftime(\"%m-%d-%H\", time.localtime(time.time()))\n",
    "ckpt_name=save_suffix+\"-\"+str(epochs)+\"epochs-\"+str(run_time)+\"times\"\n",
    "\n",
    "check_path = os.path.join(save_dir,'ckpt',ckpt_name)\n",
    "hist_path = os.path.join(save_dir, \"hist\",ckpt_name+\".json\")\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "filepath=f'{check_path}.keras',\n",
    "monitor='categorical_accuracy',\n",
    "mode='max',\n",
    "save_freq='epoch',\n",
    "save_best_only=True)\n",
    "\n",
    "\n",
    "start_person=1\n",
    "start_count=1\n",
    "current_word=\" \"\n",
    "\n",
    "for i in range(1,17):\n",
    "    l_raws=[]\n",
    "    r_raws=[]\n",
    "    p_raws=[]\n",
    "    y_raws=[]\n",
    "    loss_weights_raws=[]\n",
    "    if start_person>i:\n",
    "        continue\n",
    "    for k in range(1,run_time+1):\n",
    "        if start_count>k:\n",
    "            continue\n",
    "        elif start_count==k:\n",
    "            start_count=0\n",
    "        for j in sorted(os.listdir(os.path.join(preprocess.getoutputdir(),str(i)))):\n",
    "            if len(l_raws)==0:\n",
    "                current_word = j\n",
    "            else:\n",
    "                end_word = j\n",
    "            l_raw, r_raw, p_raw, y_raw, loss_weights_raw = preprocess.load_data(f\"{i}/{j}\")\n",
    "            l_raws.append(l_raw)\n",
    "            r_raws.append(r_raw)\n",
    "            p_raws.append(p_raw)\n",
    "            y_raws.append(y_raw)\n",
    "            loss_weights_raws.append(loss_weights_raw)\n",
    "            # print(p_raw.shape)\n",
    "            # break\n",
    "            if len(l_raws)>=load_size:\n",
    "                with open(os.path.join('logs',ckpt_name+'.txt'), 'a') as logs:\n",
    "                    logs.write(f'{ time.strftime(\"%H-%M-%S\", time.localtime(time.time()))}:{k}) person:{i} : {current_word} ~ {end_word}\\n')  # 한 줄 쓰기\n",
    "                    l_train, each = SLR_model_GRU.serialize(l_raws)\n",
    "                    r_train, each = SLR_model_GRU.serialize(r_raws)\n",
    "                    p_train, each, sample_weights = SLR_model_GRU.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                    x_train = (l_train, r_train, p_train)\n",
    "                    \n",
    "                    y_train = np.repeat(y_raws, each)\n",
    "                    y_train = SLR_model_GRU.encode_onehot2d(y_train)\n",
    "                    \n",
    "                    dataset = SLR_model_GRU.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                    hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "                    with open(hist_path, 'w') as file:\n",
    "                        json.dump(hist.history, file)\n",
    "                    l_raws.clear()\n",
    "                    r_raws.clear()\n",
    "                    p_raws.clear()\n",
    "                    y_raws.clear()\n",
    "                    loss_weights_raws.clear()\n",
    "            if len(l_raws)>0:\n",
    "                l_train, each = SLR_model_GRU.serialize(l_raws)\n",
    "                r_train, each = SLR_model_GRU.serialize(r_raws)\n",
    "                p_train, each, sample_weights = SLR_model_GRU.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                x_train = (l_train, r_train, p_train)\n",
    "                \n",
    "                y_train = np.repeat(y_raws, each)\n",
    "                y_train = SLR_model_GRU.encode_onehot2d(y_train)\n",
    "                \n",
    "                \n",
    "                dataset = SLR_model_GRU.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "                with open(hist_path, 'w') as file:\n",
    "                    json.dump(hist.history, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217de3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e74e08d3b98fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:59:17.273289Z",
     "start_time": "2024-11-06T11:59:17.262280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "load_size = 3000 # number of data to be loaded at once\n",
    "epochs = 50\n",
    "run_time = 2\n",
    "batch_size = 16\n",
    "save_dir = \"saves\"\n",
    "load_dir = \"saves\"\n",
    "# load_path = \"C:/Users/jerry/Desktop/hly/2024-2/cap/BN_SLR/BomNae-SLR/checkpoints.keras\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a3ca3f4b1820c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:59:17.382288Z",
     "start_time": "2024-11-06T11:59:17.372290Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SLR_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96948ee065658f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 세이브된 모델 로드\n",
    "# load_path = os.join(load_dir, \"check_00-00-00.keras\")\n",
    "# model = SLR_model.load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac698d86f47869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 안될때 (강제중지 + 초기화)\n",
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bad08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"saves_GRU\",\"hist\",\"11-06-21-50epochs-2times.json\"), 'r') as f:\n",
    "    hist = json.load(f)\n",
    "\n",
    "    plt.plot(range(len(hist['loss'])), hist['loss'])\n",
    "    plt.scatter(range(len(hist['loss'])), hist['loss'])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend([\"loss\"])\n",
    "    plt.ylim((0,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981331a75aa7e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:02:15.876445Z",
     "start_time": "2024-10-14T14:02:15.873228Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(os.listdir(os.path.join(preprocess.getoutputdir(),str(15))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.metrics_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
