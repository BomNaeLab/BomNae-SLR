{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358d46b6480881db",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 17:04:43.236296: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 17:04:43.242038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 17:04:43.249073: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 17:04:43.251311: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 17:04:43.256749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 17:04:43.900361: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-11-06 17:04:43.900383: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: shdUbuntu\n",
      "2024-11-06 17:04:43.900386: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: shdUbuntu\n",
      "2024-11-06 17:04:43.900427: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 560.35.3\n",
      "2024-11-06 17:04:43.900439: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  560.35.03  Release Build  (dvs-builder@U16-I1-N07-12-3)  Fri Aug 16 21:42:42 UTC 2024\n",
      "GCC version:  gcc version 13.2.0 (Ubuntu 13.2.0-23ubuntu4) \n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "import tensorflow as tf\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "import preprocess\n",
    "# import preprocess as prep\n",
    "import SLR_model\n",
    "import SLR_model_GRU\n",
    "import numpy as np\n",
    "# from preprocess import person\n",
    "\n",
    "\n",
    "\n",
    "# model will output multiple(5) results/sec, how are we gonna handle it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8634f5da68e224",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "load_size = 3000 # number of data to be loaded at once\n",
    "epochs = 50\n",
    "run_time=2\n",
    "batch_size = 16\n",
    "save_dir = \"saves_GRU\"\n",
    "load_dir = \"saves_GRU\"\n",
    "model = SLR_model_GRU.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5961b31976929d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:23:21.058102Z",
     "start_time": "2024-11-02T13:23:18.527158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3691/3691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - categorical_accuracy: 0.0014 - loss: 2.2689\n",
      "Epoch 2/50\n",
      "\u001b[1m3691/3691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - categorical_accuracy: 0.0018 - loss: 2.2203\n",
      "Epoch 3/50\n",
      "\u001b[1m2924/3691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - categorical_accuracy: 0.0015 - loss: 2.1683"
     ]
    }
   ],
   "source": [
    " # reload model file\n",
    "end_file=preprocess.getoutputdir()\n",
    "\n",
    "save_suffix = time.strftime(\"%m-%d-%H\", time.localtime(time.time()))\n",
    "ckpt_name=save_suffix+\"-\"+str(epochs)+\"epochs-\"+str(run_time)+\"times\"\n",
    "\n",
    "check_path = os.path.join(save_dir,'ckpt',ckpt_name)\n",
    "hist_path = os.path.join(save_dir, \"hist\",ckpt_name+\".json\")\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "filepath=f'{check_path}.keras',\n",
    "monitor='categorical_accuracy',\n",
    "mode='max',\n",
    "save_freq='epoch',\n",
    "save_best_only=True)\n",
    "\n",
    "\n",
    "start_person=1\n",
    "start_count=1\n",
    "start_word=\" \"\n",
    "with open(os.path.join('logs',ckpt_name+'.txt'), 'a') as logs:\n",
    "    for i in range(1,17):\n",
    "        l_raws=[]\n",
    "        r_raws=[]\n",
    "        p_raws=[]\n",
    "        y_raws=[]\n",
    "        loss_weights_raws=[]\n",
    "        if start_person>i:\n",
    "            continue\n",
    "        for k in range(1,run_time+1):\n",
    "            if start_count>k:\n",
    "                continue\n",
    "            elif start_count==k:\n",
    "                start_count=0\n",
    "            for j in sorted(os.listdir(os.path.join(preprocess.getoutputdir(),str(i)))):\n",
    "                if len(l_raws)==0:\n",
    "                    start_word = j\n",
    "                else:\n",
    "                    end_word = j\n",
    "                l_raw, r_raw, p_raw, y_raw, loss_weights_raw = preprocess.load_data(f\"{i}/{j}\")\n",
    "                l_raws.append(l_raw)\n",
    "                r_raws.append(r_raw)\n",
    "                p_raws.append(p_raw)\n",
    "                y_raws.append(y_raw)\n",
    "                loss_weights_raws.append(loss_weights_raw)\n",
    "                # print(p_raw.shape)\n",
    "                # break\n",
    "                if len(l_raws)>=load_size:\n",
    "                    \n",
    "                    logs.write(f'{ time.strftime(\"%H-%M-%S\", time.localtime(time.time()))}:{k}) person:{i} : {start_word} ~ {end_word}\\n')  # 한 줄 쓰기\n",
    "                    l_train, each = SLR_model_GRU.serialize(l_raws)\n",
    "                    r_train, each = SLR_model_GRU.serialize(r_raws)\n",
    "                    p_train, each, sample_weights = SLR_model_GRU.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                    x_train = (l_train, r_train, p_train)\n",
    "                    \n",
    "                    y_train = np.repeat(y_raws, each)\n",
    "                    y_train = SLR_model_GRU.encode_onehot2d(y_train)\n",
    "                    \n",
    "                    dataset = SLR_model_GRU.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                    hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                    \n",
    "                    # model.save(os.path.join(save_dir, 'model', f\"{ckpt_name}.h5\"))\n",
    "\n",
    "                    # converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "                    # tflite_model = converter.convert()\n",
    "                    # with open(os.path.join(save_dir, 'model', f\"{ckpt_name}.tflite\"), 'wb') as tflite_file:\n",
    "                    #     tflite_file.write(tflite_model)\n",
    "\n",
    "                    with open(hist_path, 'w') as file:\n",
    "                        json.dump(hist.history, file)\n",
    "                    l_raws.clear()\n",
    "                    r_raws.clear()\n",
    "                    p_raws.clear()\n",
    "                    y_raws.clear()\n",
    "                    loss_weights_raws.clear()\n",
    "            if len(l_raws)>0:\n",
    "                l_train, each = SLR_model_GRU.serialize(l_raws)\n",
    "                r_train, each = SLR_model_GRU.serialize(r_raws)\n",
    "                p_train, each, sample_weights = SLR_model_GRU.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "                x_train = (l_train, r_train, p_train)\n",
    "                \n",
    "                y_train = np.repeat(y_raws, each)\n",
    "                y_train = SLR_model_GRU.encode_onehot2d(y_train)\n",
    "                \n",
    "                \n",
    "                dataset = SLR_model_GRU.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "                hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                \n",
    "                # model.save(os.path.join(save_dir, 'model', f\"{ckpt_name}.h5\"))\n",
    "\n",
    "                # converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "                # tflite_model = converter.convert()\n",
    "                # with open(os.path.join(save_dir, 'model', f\"{ckpt_name}.tflite\"), 'wb') as tflite_file:\n",
    "                #     tflite_file.write(tflite_model)\n",
    "\n",
    "                with open(hist_path, 'w') as file:\n",
    "                    json.dump(hist.history, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e74e08d3b98fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configurations\n",
    "# load_size = 3000 # number of data to be loaded at once\n",
    "# epochs = 50\n",
    "# batch_size = 16\n",
    "# save_dir = \"saves\"\n",
    "# load_dir = \"saves\"\n",
    "# # load_path = \"C:/Users/jerry/Desktop/hly/2024-2/cap/BN_SLR/BomNae-SLR/checkpoints.keras\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589a3ca3f4b1820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SLR_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdb6cd65c6b35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # reload model file\n",
    "# importlib.reload(SLR_model)\n",
    "# importlib.reload(preprocess)\n",
    "# end_file=preprocess.getoutputdir()\n",
    "\n",
    "# save_suffix = time.strftime(\"_%d-%H-%M\", time.localtime(time.time()))\n",
    "\n",
    "\n",
    "# # checkTime=\"_14-16-16\"\n",
    "# # check_path = os.path.join(save_dir, \"check\"+checkTime)\n",
    "# # hist_path = os.path.join(save_dir, \"hist\"+checkTime+\".json\")\n",
    "\n",
    "# check_path = os.path.join(save_dir, \"check\"+save_suffix+\"_50epochs_3time\")\n",
    "# hist_path = os.path.join(save_dir, \"hist\"+save_suffix+\".json\")\n",
    "\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "# filepath=f'{check_path}.keras',\n",
    "# monitor='categorical_accuracy',\n",
    "# mode='max',\n",
    "# save_freq='epoch',\n",
    "# save_best_only=True)\n",
    "\n",
    "\n",
    "# start_person=1\n",
    "# start_count=1\n",
    "# start_word=\" \"\n",
    "# for i in range(1,17):\n",
    "#     l_raws=[]\n",
    "#     r_raws=[]\n",
    "#     p_raws=[]\n",
    "#     y_raws=[]\n",
    "#     loss_weights_raws=[]\n",
    "#     if start_person>i:\n",
    "#         continue\n",
    "#     for k in range(1,2):\n",
    "#         if start_count>k:\n",
    "#             continue\n",
    "#         elif start_count==k:\n",
    "#             start_count=0\n",
    "#         for j in sorted(os.listdir(os.path.join(preprocess.getoutputdir(),str(i)))):\n",
    "#             if len(l_raws)==0:\n",
    "#                 start_word = j\n",
    "#             else:\n",
    "#                 end_word = j\n",
    "#             l_raw, r_raw, p_raw, y_raw, loss_weights_raw = preprocess.load_data(f\"{i}/{j}\")\n",
    "#             l_raws.append(l_raw)\n",
    "#             r_raws.append(r_raw)\n",
    "#             p_raws.append(p_raw)\n",
    "#             y_raws.append(y_raw)\n",
    "#             loss_weights_raws.append(loss_weights_raw)\n",
    "    \n",
    "#             if len(l_raws)>=load_size:\n",
    "#                 with open(str(i)+'training_log.txt', 'a') as logs:\n",
    "#                     logs.write(f'{ time.strftime(\"%H-%M-%S\", time.localtime(time.time()))}:{k}) person:{i} : {start_word} ~ {end_word}\\n')  # 한 줄 쓰기\n",
    "#                 print(f\"{k}) person:{i} : {start_word} ~ {end_word}\\n\\n\\n\\n\")\n",
    "#                 l_train, each = SLR_model.serialize(l_raws)\n",
    "#                 r_train, each = SLR_model.serialize(r_raws)\n",
    "#                 p_train, each, sample_weights = SLR_model.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "#                 x_train = (l_train, r_train, p_train)\n",
    "                \n",
    "#                 y_train = np.repeat(y_raws, each)\n",
    "#                 y_train = SLR_model.encode_onehot2d(y_train)\n",
    "    \n",
    "#                 dataset = SLR_model.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "#                 hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "                \n",
    "#                 with open(hist_path, 'w') as file:\n",
    "#                     json.dump(hist.history, file)\n",
    "#                 l_raws.clear()\n",
    "#                 r_raws.clear()\n",
    "#                 p_raws.clear()\n",
    "#                 y_raws.clear()\n",
    "#                 loss_weights_raws.clear()\n",
    "#         if len(l_raws)>0:\n",
    "#             l_train, each = SLR_model.serialize(l_raws)\n",
    "#             r_train, each = SLR_model.serialize(r_raws)\n",
    "#             p_train, each, sample_weights = SLR_model.serialize(p_raws, stride=2, loss_weights_list=loss_weights_raws)\n",
    "#             x_train = (l_train, r_train, p_train)\n",
    "            \n",
    "#             y_train = np.repeat(y_raws, each)\n",
    "#             y_train = SLR_model.encode_onehot2d(y_train)\n",
    "    \n",
    "#             dataset = SLR_model.convert_to_dataset(x_train, y_train, batch_size, sample_weights)\n",
    "#             hist = model.fit(dataset, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "            \n",
    "#             with open(hist_path, 'w') as file:\n",
    "#                 json.dump(hist.history, file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9501d29fe7faee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand input shape: (batch, time, h, w, channels)\n",
    "# pose input shape: (batch, time, channel, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96948ee065658f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 세이브된 모델 로드\n",
    "# load_path = os.join(load_dir, \"check_00-00-00.keras\")\n",
    "# model = SLR_model.load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac698d86f47869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 안될때 (강제중지 + 초기화)\n",
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6755ee45fa0f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_raws=[]\n",
    "# r_raws=[]\n",
    "# p_raws=[]\n",
    "# y_raws=[]\n",
    "\n",
    "\n",
    "# save_suffix = time.strftime(\"_%d-%H-%M\", time.localtime(time.time()))\n",
    "# check_path = os.path.join(save_dir, \"check\"+save_suffix)\n",
    "# hist_path = os.path.join(save_dir, \"hist\"+save_suffix+\".json\")\n",
    "\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=f'{check_path}.keras',\n",
    "#     monitor='binary_accuracy',\n",
    "#     mode='max',\n",
    "#     save_freq='epoch',\n",
    "#     save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "# l_raw, r_raw, p_raw, y_raw = preprocess.load_data(f\"1/1\")\n",
    "\n",
    "# l_raws.append(l_raw)\n",
    "# r_raws.append(r_raw)\n",
    "# p_raws.append(p_raw)\n",
    "# y_raws.append(y_raw)\n",
    "\n",
    "\n",
    "# l_train, each = SLR_model.serialize(l_raws)\n",
    "# r_train, each = SLR_model.serialize(r_raws)\n",
    "# p_train, each = SLR_model.serialize(p_raws, stride=2)\n",
    "# y_train = np.repeat(y_raws, each)\n",
    "# y_train = SLR_model.num_arr2bin(y_train, 12)\n",
    "# x_train = (l_train, r_train, p_train)\n",
    "\n",
    "# # dataset = SLR_model.convert_to_dataset(x_train, y_train, batch_size)\n",
    "# hist = model.fit(x_train, y_train, batch_size = 2, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# with open(hist_path, 'w') as file:\n",
    "#     json.dump(hist.history, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bad08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(len(hist.history['loss'])), hist.history['loss'])\n",
    "# plt.scatter(range(len(hist.history['loss'])), hist.history['loss'])\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.legend([\"loss\"])\n",
    "# plt.ylim((0,1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2981331a75aa7e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:02:15.876445Z",
     "start_time": "2024-10-14T14:02:15.873228Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(os.listdir(os.path.join(preprocess.getoutputdir(),str(15))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c47fff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.metrics_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BomNae-SLR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
